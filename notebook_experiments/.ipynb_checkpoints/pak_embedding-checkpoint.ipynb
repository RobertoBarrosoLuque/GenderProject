{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"/Users/rukhshan/Documents/GitHub/adv_ml/GenderProject\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gensim \n",
    "import spacy \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "## NN based spanish sentiment scorer built on Tensorflow + Keras\n",
    "from pre_process.text_utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"/Users/rukhshan/Documents/GitHub/adv_ml/GenderProject/data/Pakistan/pak_extracted_combined.csv\")\n",
    "df3 = df3[['link', 'datetime', 'title', 'text', 'authors', 'summary', 'keywords']]\n",
    "df3 = df3[['link', 'datetime', 'title', 'text', 'authors', 'summary', 'keywords']]\n",
    "\n",
    "df3['newspaper'] = \"\"\n",
    "df3['newspaper'] = np.where(df3.link.str.contains(\"nation.com\"), \"nation\", df3.newspaper)\n",
    "df3['newspaper'] = np.where(df3.link.str.contains(\"news.com\"), \"news\", df3.newspaper)\n",
    "df3['newspaper'] = np.where(df3.link.str.contains(\"dawn.com\"), \"dawn\", df3.newspaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_df = df3[df3['newspaper']=='nation']\n",
    "dawn_df = df3[df3['newspaper']=='dawn']\n",
    "thenews_df = df3[df3['newspaper']=='news']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dates_df = pd.read_csv(\"/Users/rukhshan/Documents/GitHub/final-project-rukhshanarifm/news_datetime.csv\")\n",
    "news_final_df = news_dates_df.merge(thenews_df, on='link')\n",
    "dawn_df['datetime'] = pd.to_datetime(dawn_df['datetime'])\n",
    "nation_2020ow = nation_df[(nation_df['datetime'].str.contains(\"2020\")) | (nation_df['datetime'].str.contains(\"2021\"))]\n",
    "dawn_2020ow = dawn_df[(dawn_df['datetime'].dt.year == 2020) | (dawn_df['datetime'].dt.year == 2021)]\n",
    "news_2020ow = news_2020ow = news_final_df[(news_final_df['datetime_x'].str.contains(\"2020\")) | (news_final_df['datetime_x'].str.contains(\"2021\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_2020ow[\"title\"] = news_2020ow[\"title\"].str.pad(1, side ='right', fillchar =' ')\n",
    "news_2020ow[\"complete_text\"]  = news_2020ow.title.str.cat(news_2020ow.text).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnlp_english = spacy.load(\"en_core_web_sm\")\\n\\nnews_2020ow[\\'tokenized_sentences\\'] = news_2020ow[\\'complete_text\\'].apply(lambda x: \\n                                                                      [word_tokenize(s, model=nlp_english)\\n                                                                      for s in sent_tokenize(x)])\\nnews_2020ow[\\'normalized_sentences\\'] = news_2020ow[\\'tokenized_sentences\\'].apply(lambda x:\\n                                                                            [normalize_tokens(s, model=nlp_english)\\n                                                                            for s in x])\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize and tokenize all documents\n",
    "\n",
    "nlp_english = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "news_2020ow['tokenized_sentences'] = news_2020ow['complete_text'].apply(lambda x: \n",
    "                                                                      [word_tokenize(s, model=nlp_english)\n",
    "                                                                      for s in sent_tokenize(x)])\n",
    "news_2020ow['normalized_sentences'] = news_2020ow['tokenized_sentences'].apply(lambda x:\n",
    "                                                                            [normalize_tokens(s, model=nlp_english)\n",
    "                                                                            for s in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'normalized_sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'normalized_sentences'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-30f1fe74ff7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews_w2v\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_2020ow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normalized_sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'normalized_sentences'"
     ]
    }
   ],
   "source": [
    "news_w2v  = gensim.models.word2vec.Word2Vec(news_2020ow['normalized_sentences'].sum(),window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gender', 0.9991583228111267),\n",
       " ('based', 0.9987853169441223),\n",
       " ('domestic', 0.9964468479156494),\n",
       " ('empowerment', 0.9953795075416565),\n",
       " ('rights', 0.9945817589759827),\n",
       " ('violence', 0.9940029978752136),\n",
       " ('forms', 0.9933726191520691),\n",
       " ('country', 0.992571234703064),\n",
       " ('child', 0.9925147294998169),\n",
       " ('harassment', 0.9923556447029114)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_w2v.wv.most_similar(\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.9984163641929626),\n",
       " ('man', 0.9973467588424683),\n",
       " ('minor', 0.9961004257202148),\n",
       " ('allegedly', 0.9958158135414124),\n",
       " ('kidnapped', 0.99568772315979),\n",
       " ('raping', 0.995495080947876),\n",
       " ('limits', 0.9948391318321228),\n",
       " ('arrested', 0.9945178627967834),\n",
       " ('abducted', 0.9944660067558289),\n",
       " ('teenage', 0.994373083114624)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_w2v.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndawn_2020ow[\"title\"] = dawn_2020ow[\"title\"].str.pad(1, side =\\'right\\', fillchar =\\' \\')\\ndawn_2020ow[\"complete_text\"]  = dawn_2020ow.title.str.cat(dawn_2020ow.text).astype(str)\\ndawn_2020ow[\\'tokenized_sentences\\'] = dawn_2020ow[\\'complete_text\\'].apply(lambda x: \\n                                                                      [word_tokenize(s, model=nlp_english)\\n                                                                      for s in sent_tokenize(x)])\\ndawn_2020ow[\\'normalized_sentences\\'] = dawn_2020ow[\\'tokenized_sentences\\'].apply(lambda x:\\n                                                                            [normalize_tokens(s, model=nlp_english)\\n                                                                            for s in x])\\ndawn_w2v  = gensim.models.word2vec.Word2Vec(dawn_2020ow[\\'normalized_sentences\\'].sum(),window=10)\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dawn_2020ow[\"title\"] = dawn_2020ow[\"title\"].str.pad(1, side ='right', fillchar =' ')\n",
    "dawn_2020ow[\"complete_text\"]  = dawn_2020ow.title.str.cat(dawn_2020ow.text).astype(str)\n",
    "dawn_2020ow['tokenized_sentences'] = dawn_2020ow['complete_text'].apply(lambda x: \n",
    "                                                                      [word_tokenize(s, model=nlp_english)\n",
    "                                                                      for s in sent_tokenize(x)])\n",
    "dawn_2020ow['normalized_sentences'] = dawn_2020ow['tokenized_sentences'].apply(lambda x:\n",
    "                                                                            [normalize_tokens(s, model=nlp_english)\n",
    "                                                                            for s in x])\n",
    "dawn_w2v  = gensim.models.word2vec.Word2Vec(dawn_2020ow['normalized_sentences'].sum(),window=10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ali', 0.9998335242271423),\n",
       " ('said', 0.9998325705528259),\n",
       " ('violence', 0.9998313784599304),\n",
       " ('state', 0.9998263120651245),\n",
       " ('children', 0.9998194575309753),\n",
       " ('law', 0.9998192191123962),\n",
       " ('minister', 0.99981689453125),\n",
       " ('country', 0.9998145699501038),\n",
       " ('justice', 0.9998140335083008),\n",
       " ('police', 0.9998077154159546)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dawn_w2v.wv.most_similar(\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 0.9998308420181274),\n",
       " ('police', 0.9997838139533997),\n",
       " ('ali', 0.9997808337211609),\n",
       " ('family', 0.9997771382331848),\n",
       " ('sindh', 0.9997691512107849),\n",
       " ('pakistan', 0.9997639060020447),\n",
       " ('government', 0.9997497200965881),\n",
       " ('karachi', 0.9997479319572449),\n",
       " ('suspect', 0.9997455477714539),\n",
       " ('justice', 0.9997419118881226)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dawn_w2v.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnation_2020ow[\"title\"] = nation_2020ow[\"title\"].str.pad(1, side =\\'right\\', fillchar =\\' \\')\\nnation_2020ow[\"complete_text\"]  = nation_2020ow.title.str.cat(nation_2020ow.text).astype(str)\\nnation_2020ow[\\'tokenized_sentences\\'] = nation_2020ow[\\'complete_text\\'].apply(lambda x: \\n                                                                      [word_tokenize(s, model=nlp_english)\\n                                                                      for s in sent_tokenize(x)])\\nnation_2020ow[\\'normalized_sentences\\'] = nation_2020ow[\\'tokenized_sentences\\'].apply(lambda x:\\n                                                                            [normalize_tokens(s, model=nlp_english)\\n                                                                            for s in x])\\nnation_w2v  = gensim.models.word2vec.Word2Vec(nation_2020ow[\\'normalized_sentences\\'].sum(),window=10)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nation_2020ow[\"title\"] = nation_2020ow[\"title\"].str.pad(1, side ='right', fillchar =' ')\n",
    "nation_2020ow[\"complete_text\"]  = nation_2020ow.title.str.cat(nation_2020ow.text).astype(str)\n",
    "nation_2020ow['tokenized_sentences'] = nation_2020ow['complete_text'].apply(lambda x: \n",
    "                                                                      [word_tokenize(s, model=nlp_english)\n",
    "                                                                      for s in sent_tokenize(x)])\n",
    "nation_2020ow['normalized_sentences'] = nation_2020ow['tokenized_sentences'].apply(lambda x:\n",
    "                                                                            [normalize_tokens(s, model=nlp_english)\n",
    "                                                                            for s in x])\n",
    "nation_w2v  = gensim.models.word2vec.Word2Vec(nation_2020ow['normalized_sentences'].sum(),window=10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pakistan', 0.9998694658279419),\n",
       " ('society', 0.9998290538787842),\n",
       " ('government', 0.9998155236244202),\n",
       " ('country', 0.9998108744621277),\n",
       " ('laws', 0.9998046159744263),\n",
       " ('social', 0.9998027682304382),\n",
       " ('children', 0.999799907207489),\n",
       " ('gender', 0.9997987151145935),\n",
       " ('victims', 0.9997978806495667),\n",
       " ('child', 0.9997929334640503)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_w2v.wv.most_similar(\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.9997513294219971),\n",
       " ('area', 0.99973464012146),\n",
       " ('killed', 0.999720573425293),\n",
       " ('man', 0.9997128248214722),\n",
       " ('police', 0.9997102618217468),\n",
       " ('allegedly', 0.9997049570083618),\n",
       " ('year', 0.9997023344039917),\n",
       " ('woman', 0.9996995329856873),\n",
       " ('lahore', 0.9996957778930664),\n",
       " ('raped', 0.999686598777771)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_w2v.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
