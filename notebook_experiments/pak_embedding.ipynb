{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df3 = pd.read_csv(\"/Users/rukhshan/Documents/GitHub/adv_ml/GenderProject/data/Pakistan/pak_extracted_combined.csv\")\n",
    "df3 = df3[['link', 'datetime', 'title', 'text', 'authors', 'summary', 'keywords']]\n",
    "df3 = df3[['link', 'datetime', 'title', 'text', 'authors', 'summary', 'keywords']]\n",
    "\n",
    "df3['newspaper'] = \"\"\n",
    "df3['newspaper'] = np.where(df3.link.str.contains(\"nation.com\"), \"nation\", df3.newspaper)\n",
    "df3['newspaper'] = np.where(df3.link.str.contains(\"news.com\"), \"news\", df3.newspaper)\n",
    "df3['newspaper'] = np.where(df3.link.str.contains(\"dawn.com\"), \"dawn\", df3.newspaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_df = df3[df3['newspaper']=='nation']\n",
    "dawn_df = df3[df3['newspaper']=='dawn']\n",
    "thenews_df = df3[df3['newspaper']=='news']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rukhshan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "news_dates_df = pd.read_csv(\"/Users/rukhshan/Documents/GitHub/final-project-rukhshanarifm/news_datetime.csv\")\n",
    "news_final_df = news_dates_df.merge(thenews_df, on='link')\n",
    "dawn_df['datetime'] = pd.to_datetime(dawn_df['datetime'])\n",
    "nation_2020ow = nation_df[(nation_df['datetime'].str.contains(\"2020\")) | (nation_df['datetime'].str.contains(\"2021\"))]\n",
    "dawn_2020ow = dawn_df[(dawn_df['datetime'].dt.year == 2020) | (dawn_df['datetime'].dt.year == 2021)]\n",
    "news_2020ow = news_2020ow = news_final_df[(news_final_df['datetime_x'].str.contains(\"2020\")) | (news_final_df['datetime_x'].str.contains(\"2021\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"/Users/rukhshan/Documents/GitHub/adv_ml/GenderProject\")\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import gensim \n",
    "import spacy \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "## NN based spanish sentiment scorer built on Tensorflow + Keras\n",
    "from pre_process.text_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rukhshan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/rukhshan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "news_2020ow[\"title\"] = news_2020ow[\"title\"].str.pad(1, side ='right', fillchar =' ')\n",
    "news_2020ow[\"complete_text\"]  = news_2020ow.title.str.cat(news_2020ow.text).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnlp_english = spacy.load(\"en_core_web_sm\")\\n\\nnews_2020ow[\\'tokenized_sentences\\'] = news_2020ow[\\'complete_text\\'].apply(lambda x: \\n                                                                      [word_tokenize(s, model=nlp_english)\\n                                                                      for s in sent_tokenize(x)])\\nnews_2020ow[\\'normalized_sentences\\'] = news_2020ow[\\'tokenized_sentences\\'].apply(lambda x:\\n                                                                            [normalize_tokens(s, model=nlp_english)\\n                                                                            for s in x])\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize and tokenize all documents\n",
    "\n",
    "'''\n",
    "nlp_english = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "news_2020ow['tokenized_sentences'] = news_2020ow['complete_text'].apply(lambda x: \n",
    "                                                                      [word_tokenize(s, model=nlp_english)\n",
    "                                                                      for s in sent_tokenize(x)])\n",
    "news_2020ow['normalized_sentences'] = news_2020ow['tokenized_sentences'].apply(lambda x:\n",
    "                                                                            [normalize_tokens(s, model=nlp_english)\n",
    "                                                                            for s in x])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_w2v  = gensim.models.word2vec.Word2Vec(news_2020ow['normalized_sentences'].sum(),window=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gender', 0.9991583228111267),\n",
       " ('based', 0.9987853169441223),\n",
       " ('domestic', 0.9964468479156494),\n",
       " ('empowerment', 0.9953795075416565),\n",
       " ('rights', 0.9945817589759827),\n",
       " ('violence', 0.9940029978752136),\n",
       " ('forms', 0.9933726191520691),\n",
       " ('country', 0.992571234703064),\n",
       " ('child', 0.9925147294998169),\n",
       " ('harassment', 0.9923556447029114)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_w2v.wv.most_similar(\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.9984163641929626),\n",
       " ('man', 0.9973467588424683),\n",
       " ('minor', 0.9961004257202148),\n",
       " ('allegedly', 0.9958158135414124),\n",
       " ('kidnapped', 0.99568772315979),\n",
       " ('raping', 0.995495080947876),\n",
       " ('limits', 0.9948391318321228),\n",
       " ('arrested', 0.9945178627967834),\n",
       " ('abducted', 0.9944660067558289),\n",
       " ('teenage', 0.994373083114624)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_w2v.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndawn_2020ow[\"title\"] = dawn_2020ow[\"title\"].str.pad(1, side =\\'right\\', fillchar =\\' \\')\\ndawn_2020ow[\"complete_text\"]  = dawn_2020ow.title.str.cat(dawn_2020ow.text).astype(str)\\ndawn_2020ow[\\'tokenized_sentences\\'] = dawn_2020ow[\\'complete_text\\'].apply(lambda x: \\n                                                                      [word_tokenize(s, model=nlp_english)\\n                                                                      for s in sent_tokenize(x)])\\ndawn_2020ow[\\'normalized_sentences\\'] = dawn_2020ow[\\'tokenized_sentences\\'].apply(lambda x:\\n                                                                            [normalize_tokens(s, model=nlp_english)\\n                                                                            for s in x])\\ndawn_w2v  = gensim.models.word2vec.Word2Vec(dawn_2020ow[\\'normalized_sentences\\'].sum(),window=10)\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dawn_2020ow[\"title\"] = dawn_2020ow[\"title\"].str.pad(1, side ='right', fillchar =' ')\n",
    "dawn_2020ow[\"complete_text\"]  = dawn_2020ow.title.str.cat(dawn_2020ow.text).astype(str)\n",
    "dawn_2020ow['tokenized_sentences'] = dawn_2020ow['complete_text'].apply(lambda x: \n",
    "                                                                      [word_tokenize(s, model=nlp_english)\n",
    "                                                                      for s in sent_tokenize(x)])\n",
    "dawn_2020ow['normalized_sentences'] = dawn_2020ow['tokenized_sentences'].apply(lambda x:\n",
    "                                                                            [normalize_tokens(s, model=nlp_english)\n",
    "                                                                            for s in x])\n",
    "dawn_w2v  = gensim.models.word2vec.Word2Vec(dawn_2020ow['normalized_sentences'].sum(),window=10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ali', 0.9998335242271423),\n",
       " ('said', 0.9998325705528259),\n",
       " ('violence', 0.9998313784599304),\n",
       " ('state', 0.9998263120651245),\n",
       " ('children', 0.9998194575309753),\n",
       " ('law', 0.9998192191123962),\n",
       " ('minister', 0.99981689453125),\n",
       " ('country', 0.9998145699501038),\n",
       " ('justice', 0.9998140335083008),\n",
       " ('police', 0.9998077154159546)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dawn_w2v.wv.most_similar(\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 0.9998308420181274),\n",
       " ('police', 0.9997838139533997),\n",
       " ('ali', 0.9997808337211609),\n",
       " ('family', 0.9997771382331848),\n",
       " ('sindh', 0.9997691512107849),\n",
       " ('pakistan', 0.9997639060020447),\n",
       " ('government', 0.9997497200965881),\n",
       " ('karachi', 0.9997479319572449),\n",
       " ('suspect', 0.9997455477714539),\n",
       " ('justice', 0.9997419118881226)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dawn_w2v.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnation_2020ow[\"title\"] = nation_2020ow[\"title\"].str.pad(1, side =\\'right\\', fillchar =\\' \\')\\nnation_2020ow[\"complete_text\"]  = nation_2020ow.title.str.cat(nation_2020ow.text).astype(str)\\nnation_2020ow[\\'tokenized_sentences\\'] = nation_2020ow[\\'complete_text\\'].apply(lambda x: \\n                                                                      [word_tokenize(s, model=nlp_english)\\n                                                                      for s in sent_tokenize(x)])\\nnation_2020ow[\\'normalized_sentences\\'] = nation_2020ow[\\'tokenized_sentences\\'].apply(lambda x:\\n                                                                            [normalize_tokens(s, model=nlp_english)\\n                                                                            for s in x])\\nnation_w2v  = gensim.models.word2vec.Word2Vec(nation_2020ow[\\'normalized_sentences\\'].sum(),window=10)\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nation_2020ow[\"title\"] = nation_2020ow[\"title\"].str.pad(1, side ='right', fillchar =' ')\n",
    "nation_2020ow[\"complete_text\"]  = nation_2020ow.title.str.cat(nation_2020ow.text).astype(str)\n",
    "nation_2020ow['tokenized_sentences'] = nation_2020ow['complete_text'].apply(lambda x: \n",
    "                                                                      [word_tokenize(s, model=nlp_english)\n",
    "                                                                      for s in sent_tokenize(x)])\n",
    "nation_2020ow['normalized_sentences'] = nation_2020ow['tokenized_sentences'].apply(lambda x:\n",
    "                                                                            [normalize_tokens(s, model=nlp_english)\n",
    "                                                                            for s in x])\n",
    "nation_w2v  = gensim.models.word2vec.Word2Vec(nation_2020ow['normalized_sentences'].sum(),window=10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pakistan', 0.9998694658279419),\n",
       " ('society', 0.9998290538787842),\n",
       " ('government', 0.9998155236244202),\n",
       " ('country', 0.9998108744621277),\n",
       " ('laws', 0.9998046159744263),\n",
       " ('social', 0.9998027682304382),\n",
       " ('children', 0.999799907207489),\n",
       " ('gender', 0.9997987151145935),\n",
       " ('victims', 0.9997978806495667),\n",
       " ('child', 0.9997929334640503)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_w2v.wv.most_similar(\"women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boy', 0.9997513294219971),\n",
       " ('area', 0.99973464012146),\n",
       " ('killed', 0.999720573425293),\n",
       " ('man', 0.9997128248214722),\n",
       " ('police', 0.9997102618217468),\n",
       " ('allegedly', 0.9997049570083618),\n",
       " ('year', 0.9997023344039917),\n",
       " ('woman', 0.9996995329856873),\n",
       " ('lahore', 0.9996957778930664),\n",
       " ('raped', 0.999686598777771)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_w2v.wv.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
